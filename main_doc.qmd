---
title: "main_doc"
format: html
editor: visual
---

# Dependencies

```{r, message=FALSE}
library(mice)
library(MASS)
library(furrr)
library(purrr)
library(tidyverse)
```

# Data Simulation

## Defining data simulation function

```{r}
data_generator <- function(sample_size, seed, prop){
  set.seed(seed)
  cor_mat <- matrix(c(1, 0.5, 0.5, 
                      0.5, 1, 0.5, 
                      0.5, 0.5, 1), nrow = 3, ncol = 3)
  
  mean_vec <- c(1, 1, 1)
  
  covariates <- as.data.frame(mvrnorm(sample_size, mu = mean_vec, Sigma = cor_mat))
  covariates$V3 <- ifelse(covariates$V3 > 0, 1, 0)
  
  outcome_variable <- 0 + 
    0.32*covariates$V1 + 
    0.67*covariates$V2 +  
    0.43*covariates$V3 + 
    0.50*covariates$V2*covariates$V3 + # Interaction btwn. V2&V3
    rnorm(100,0,1)
  
  data_complete <- cbind(outcome_variable, covariates)
  data_complete$V3 <- as.factor(data_complete$V3)
  
  data_w_missing <- suppressWarnings(ampute(data_complete, prop = prop, mech = "MAR", patterns = c(0, 1, 1, 1))$amp)
  
  data_w_missing$V3 <- as.factor(ifelse(data_w_missing$V3 == 1, 0, 1))
  
  # "outcome_variable" is an outcome variable with 30% MAR. Covariates are fully observed. 
  
  # The analysis model is outcome_variable ~ V1 + V2 + V3 + V2*V3 + epsilon 
  # We are interested in estimating var(beta*V2)
  
  return(data_w_missing)
}
```

## Creating dataframe of inputs

```{r, eval = FALSE}
set.seed(971423)
N = 1e3 
# 1e3 iterations of 4x4 unique conditions. 

p_mis_vec <- c(rep(0.10, N), rep(0.30, N), rep(0.50, N), rep(0.70, N))

sample_size_vec <- c(rep(1e2, N/4), rep(1e3, N/4), rep(1e4, N/4), rep(1e5, N/4))

seed_vec <- seq.int(from = 1, to = N*4)

sample_size_vec_repeated <- rep.int(sample_size_vec, 4)

param_df <- data.frame(sample_size = sample_size_vec_repeated, prop = p_mis_vec, seed = seed_vec)

plan(multisession, workers = parallel::detectCores()-1)

sim_dfs <- future_pmap(param_df, data_generator, .progress = TRUE)

```

### Small dataset for general use

```{r}
df_w_mis <- data_generator(500, 213, 0.30)
subsamples <- vector("list", nrow(df_w_mis))
for(i in 1:nrow(df_w_mis)){
  subsamples[[i]] <- df_w_mis[-i,]}

contains_na <- map_lgl(subsamples, ~any(is.na(.x)))

for(i in 1:length(subsamples)){
  if(contains_na[[i]] == TRUE){
    subsamples[[i]] <- mice(subsamples[[i]], m = 5, method = "pmm", print=FALSE)
  } else{
    subsamples[[i]] <- subsamples[[i]]
  }
}

# Estimated vars, need to add test in case there is non-mice object 
analysis_vector<- vector("numeric", nrow(df_w_mis))

is_mice <- vector("logical", length = length(subsamples))

for(i in 1:length(subsamples)){
  is_mice[[i]] <- ifelse(class(subsamples[[i]]) == "mids", TRUE, FALSE)
}

for(i in 1:length(subsamples)){
  sample_size <- nrow(complete(subsamples[[i]],1))
  if(is_mice[[i]] == TRUE){
    analysis_vector[[i]] <- subsamples[[i]] %>%
    mice::complete("all") %>%
    map(lm, formula = outcome_variable ~ V1 + V2 + V3 + V2:V3) %>%
    pool() %>%
    broom::tidy() %>%
    dplyr::filter(term == "V2") %>%
    dplyr::select("std.error") %>%
    dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
    dplyr::select("variance") %>%
    unlist()
  } else{
    analysis_vector[[i]] <- subsamples[[i]] %>%
      lm(formula = outcome_variable ~ V1 + V2 + V3 + V2:V3, data = .) %>%
      broom::tidy() %>%
      dplyr::filter(term == "V2") %>%
      dplyr::select("std.error") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select("variance") %>%
      unlist()
  }

}

mean_var <- c()

est2 <- df_w_mis %>%
  mice(seed = 123, print = FALSE) %>%
  with(lm(outcome_variable ~ V1 + V2 + V3 + V2:V3)) %>%
  pool() %>%
  broom::tidy() %>%
      dplyr::filter(term == "V2") %>%
      dplyr::select("std.error") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select("variance") %>%
      unlist()

# True var 
true_var <- lm(outcome_variable ~ V1 + V2 + V3 + V2:V3, data = df_w_mis) %>%
  summary() %>%
  tidy() %>%
  dplyr::filter(term == "V2") %>%
  dplyr::select("std.error") %>%
  dplyr::mutate(variance = {std.error*{nrow(df_w_mis)^{1/2}}}^2) %>%
  dplyr::select("variance") %>%
  unlist()

mean_jack_estimate <- mean(analysis_vector)

bias_of_jack_estimates <- (nrow(df_w_mis)-1)*(analysis_vector - est2)

bias_corrected_jack_estimates <- analysis_vector - bias_of_jack_estimates


# quantile ci is  VERY wide, using traditional. 
UB <- round(quantile(bias_corrected_jack_estimates, probs = 1-0.05),5)
LB <- round(quantile(bias_corrected_jack_estimates, probs = 0.05),5)

# calculating variance 
jack_var <- ((nrow(df_w_mis)-1)/(nrow(df_w_mis)))*
  sum({bias_corrected_jack_estimates-mean(bias_corrected_jack_estimates)}^2)

UB <- mean(bias_corrected_jack_estimates) + 1.96(sqrt(jack_var)/sqrt(nrow(df_w_mis)))
UB <- mean(bias_corrected_jack_estimates) - 1.96(sqrt(jack_var)/sqrt(nrow(df_w_mis)))

return(data.frame(UB,
                 LB,
                 "point_estimate" =  mean(bias_corrected_jack_estimates),
                 "true_variance" = true_var) %>%
        tibble::remove_rownames(.)
        )
}
```

# Converting small example above to function for MC

```{r, eval = FALSE}
jackknife_v1 <- function(df_w_mis, seed){
  set.seed(seed)
  subsamples <- vector("list", nrow(df_w_mis))
for(i in 1:nrow(df_w_mis)){
  subsamples[[i]] <- df_w_mis[-i,]}

contains_na <- map_lgl(subsamples, ~any(is.na(.x)))

for(i in 1:length(subsamples)){
  if(contains_na[[i]] == TRUE){
    subsamples[[i]] <- mice(subsamples[[i]], m = 5, method = "pmm", print=FALSE)
  } else{
    subsamples[[i]] <- subsamples[[i]]
  }
}

# Estimated vars 
analysis_vector<- vector("numeric", nrow(df_w_mis))
for(i in 1:length(subsamples)){
  sample_size <- nrow(complete(subsamples[[i]],1))
  analysis_vector[[i]] <- subsamples[[i]] %>%
    mice::complete("all") %>%
    map(lm, formula = outcome_variable ~ V1 + V2 + V3 + V2:V3) %>%
    pool() %>%
    broom::tidy() %>%
    dplyr::filter(term == "V2") %>%
    dplyr::select("std.error") %>%
    dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
    dplyr::select("variance") %>%
    unlist()
}

UB <- round(quantile(analysis_vector, probs = 1-0.05),2)
LB <- round(quantile(analysis_vector, probs = 0.05),2)

return(data.frame(UB,
                 LB,
                 "point_estimate" =  mean(analysis_vector)))
}

# Small MC for jackknife v_1
library(parallel)
cl <- makeCluster(4)
mc_res <- mclapply(mc.cores = 4, c(1:100), function(x) jackknife_v1(df_w_mis, seed = x))
```

```{r}
jackknife_v1_bias_corrected <- function(df_w_mis, seed){
  set.seed(seed)
  subsamples <- vector("list", nrow(df_w_mis))
for(i in 1:nrow(df_w_mis)){
  subsamples[[i]] <- df_w_mis[-i,]}

contains_na <- map_lgl(subsamples, ~any(is.na(.x)))

for(i in 1:length(subsamples)){
  if(contains_na[[i]] == TRUE){
    subsamples[[i]] <- mice(subsamples[[i]], m = 5, method = "pmm", print=FALSE)
  } else{
    subsamples[[i]] <- subsamples[[i]]
  }
}

# Estimated vars, need to add test in case there is non-mice object 
analysis_vector<- vector("numeric", nrow(df_w_mis))

is_mice <- vector("logical", length = length(subsamples))

for(i in 1:length(subsamples)){
  is_mice[[i]] <- ifelse(class(subsamples[[i]]) == "mids", TRUE, FALSE)
}

for(i in 1:length(subsamples)){
  sample_size <- nrow(complete(subsamples[[i]],1))
  if(is_mice[[i]] == TRUE){
    analysis_vector[[i]] <- subsamples[[i]] %>%
    mice::complete("all") %>%
    map(lm, formula = outcome_variable ~ V1 + V2 + V3 + V2:V3) %>%
    pool() %>%
    broom::tidy() %>%
    dplyr::filter(term == "V2") %>%
    dplyr::select("std.error") %>%
    dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
    dplyr::select("variance") %>%
    unlist()
  } else{
    analysis_vector[[i]] <- subsamples[[i]] %>%
      lm(formula = outcome_variable ~ V1 + V2 + V3 + V2:V3, data = .) %>%
      broom::tidy() %>%
      dplyr::filter(term == "V2") %>%
      dplyr::select("std.error") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select("variance") %>%
      unlist()
  }

}

mean_jack_estimate <- mean(analysis_vector)

bias_of_jack_estimates <- (nrow(df_w_mis)-1)*(mean_jack_estimate - analysis_vector)

bias_corrected_jack_estimates <- analysis_vector - bias_of_jack_estimates
# True var 
true_var <- lm(outcome_variable ~ V1 + V2 + V3 + V2:V3, data = df_w_mis) %>%
  summary() %>%
  tidy() %>%
  dplyr::filter(term == "V2") %>%
  dplyr::select("std.error") %>%
  dplyr::mutate(variance = {std.error*{nrow(df_w_mis)^{1/2}}}^2) %>%
  dplyr::select("variance") %>%
  unlist()

# quantile ci is  VERY wide, using traditional. 
#UB <- round(quantile(bias_corrected_jack_estimates, probs = 1-0.05),5)
#LB <- round(quantile(bias_corrected_jack_estimates, probs = 0.05),5)

# calculating variance 
jack_var <- ((nrow(df_w_mis)-1)/(nrow(df_w_mis)))*
  sum({bias_corrected_jack_estimates-mean(bias_corrected_jack_estimates)}^2)

UB <- mean(bias_corrected_jack_estimates) + 1.96(sqrt(jack_var)/sqrt(nrow(df_w_mis)))
UB <- mean(bias_corrected_jack_estimates) - 1.96(sqrt(jack_var)/sqrt(nrow(df_w_mis)))

return(data.frame(UB,
                 LB,
                 "point_estimate" =  mean(bias_corrected_jack_estimates),
                 "true_variance" = true_var) %>%
        tibble::remove_rownames(.)
        )
}

# Small MC for jackknife v_1_bias_corrected 
# This code works for unix devices, but not for windows. 
library(parallel)
cl <- makeCluster(detectCores())
mc_res <- mclapply(mc.cores = detectCores(), c(1:50), function(x) jackknife_v1_bias_corrected(df_w_mis, seed = x))
```

```{r}
# Code below is appt. for windows machines, but slower than the one above. 
library(future.apply)
plan(multisession)

mc_res <- future_lapply(c(1:50), function(x) jackknife_v1_bias_corrected(df_w_mis, seed = x))

saveRDS(mc_res, "mc_res_v1_bias_corrected.rds")
```

```{r, warning=FALSE}
# Read in results from previous MC for jackknife v_1_bias_corrected
df <- readRDS("mc_res_v1_bias_corrected.rds") %>%
  do.call(rbind, .) %>%
  tibble::remove_rownames(.)

# CI coverage probability is very high, maybe consider traditional CI instead of percentile? 
round((sum(df$true_var < df$UB & df$true_var > df$LB) / nrow(df))*100,2)

ggplot(data = df,
           aes(point_estimate - true_var)) + 
      geom_density(aes(y = ..density..)) + 
      theme(axis.title.y = element_blank(), 
            panel.spacing=unit(1.5,"lines")) + 
      theme_bw() + 
      theme(axis.title.y = element_blank(), 
            panel.spacing=unit(1.5,"lines"), 
            strip.text = element_text(
              size = 9)) + 
      xlab("Bias of variance estimator") + 
      ggpubr::stat_overlay_normal_density(color = "red", linetype = "dashed")

ggplot(df, aes(x = c(1:nrow(df)), y = point_estimate)) + 
      geom_point(aes(color = "red")) + 
      geom_errorbar(aes(ymin = LB, ymax = UB, alpha = 1)) + 
      coord_flip() + 
      theme_bw() + 
      labs(
        x = latex2exp::TeX("$i^{th} dataset"), 
        y = latex2exp::TeX("$\\widehat{\\sigma^2}")
      ) + 
      theme(legend.position="none") + 
      geom_hline(yintercept = df$true_variance)
```
