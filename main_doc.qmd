---
title: "main_doc"
format: html
editor: visual
---

# Dependencies

```{r, message=FALSE}
library(mice)
library(MASS)
library(furrr)
library(purrr)
library(tidyverse)
library(parallel)
```

# Data Simulation

## Defining data simulation function

```{r}
data_generator <- function(sample_size, seed, prop, desired_data){
  set.seed(seed)
  cor_mat <- matrix(c(1, 0.5, 0.5, 
                      0.5, 1, 0.5, 
                      0.5, 0.5, 1), nrow = 3, ncol = 3)
  
  mean_vec <- c(1, 1, 1)
  
  covariates <- as.data.frame(mvrnorm(sample_size, mu = mean_vec, Sigma = cor_mat))
  covariates$V3 <- ifelse(covariates$V3 > 0, 1, 0)
  
  outcome_variable <- 1 + 
    0.2*covariates$V1 + 
    0.5*covariates$V2 +  
    0.8*covariates$V3 +
    rlnorm(sample_size,0,1)
  
  data_complete <- cbind(outcome_variable, covariates)
  data_complete$V3 <- as.factor(data_complete$V3)
  
  data_w_missing <- suppressWarnings(ampute(data_complete, prop = prop, mech = "MAR", patterns = c(0, 1, 1, 1))$amp)
  
  data_w_missing$V3 <- as.factor(ifelse(data_w_missing$V3 == 1, 0, 1))
  
  return(get(desired_data))
}
```

## Creating dataframe of inputs

```{r, eval = FALSE}
set.seed(971423)
N = 100
sample_size = 100
seed = 1:N
prop = 0.3
desired_data = "data_w_missing"

# Create list of data frames with missing 
cl <- makeCluster(4)

list_of_sim_data <- mclapply(mc.cores = 4, seed, function(x) data_generator(sample_size, x, prop, desired_data))

stopCluster(cl)

results_df <- do.call(rbind, mc_res)
```

# Estimator Code

```{r}
jackknife_estimator <- function(df_w_mis){
  # Creates n subsamples of n = n-1 
subsamples <- vector("list", nrow(df_w_mis))
for(i in 1:nrow(df_w_mis)){
  subsamples[[i]] <- df_w_mis[-i,]}

contains_na <- map_lgl(subsamples, ~any(is.na(.x)))

for(i in 1:length(subsamples)){
  if(contains_na[[i]] == TRUE){
    subsamples[[i]] <- mice(subsamples[[i]], m = 5, method = "pmm", print=FALSE)
  } else{
    subsamples[[i]] <- subsamples[[i]]
  }
}

# Logical test to determine presence of MICE objects 
is_mice <- vector("logical", length = length(subsamples))

for(i in 1:length(subsamples)){
  is_mice[[i]] <- ifelse(class(subsamples[[i]]) == "mids", TRUE, FALSE)
}

# Applies appt analysis depending on type 
analysis_vector <- vector("numeric", length = length(subsamples))

for(i in 1:length(subsamples)){
  sample_size <- nrow(complete(subsamples[[i]],1))
  if(is_mice[[i]] == TRUE){
    analysis_vector[[i]] <- subsamples[[i]] %>%
      mice::complete("all") %>%
      map(lm, formula = outcome_variable ~ V1 + V2 + V3) %>%
      pool() %>%
      broom::tidy() %>%
      dplyr::filter(term == "V2") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select(variance) %>%
      unlist()
  } else{
    analysis_vector[[i]] <- subsamples[[i]] %>%
      lm(formula = outcome_variable ~ V1 + V2 + V3 , data = .) %>%
      broom::tidy() %>%
      dplyr::filter(term == "V2") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select(variance) %>%
      unlist()
  }
  
}

# Jackknife point estimate
point_estimate_jackknife <- mean(analysis_vector)

UB <- quantile(analysis_vector, 0.95)
LB <- quantile(analysis_vector, 0.05)

return(data.frame("point_estimate" = point_estimate_jackknife, 
                  "UB" = UB, 
                  "LB" = LB) %>% tibble::remove_rownames())

}

lapply(c(1:2), function(x) jackknife_estimator(list_of_sim_data[[x]]))
```

# Small MC Codes

```{r}
cl <- makeCluster(4)

mc_res <- mclapply(mc.cores = 4,c(1:100), function(x) jackknife_estimator(list_of_sim_data[[x]]))

stopCluster(cl)

# Code below is appt. for windows machines, but slower than the one above. 
library(future.apply)
plan(multisession)

mc_res <- future_lapply(c(1:50), function(x) jackknife_v1_bias_corrected(df_w_mis, seed = x))

# Saving is same for both
saveRDS(mc_res, "mc_res_v1_bias_corrected.rds")
```

## Creating dataframe of inputs for variance calculations

```{r, eval = FALSE}
set.seed(971423)
N = 100
sample_size = 100
seed = 1:N
prop = 0.3
desired_data = "data_complete"

# Create list of data frames without missing (for var calculation only)
cl <- makeCluster(4)

list_of_sim_data <- mclapply(mc.cores = 4, seed, function(x) data_generator(sample_size, x, prop, desired_data))

stopCluster(cl)

cl <- makeCluster(4)

true_var <- mclapply(mc.cores = 4, list_of_sim_data,
                     function(x) lm(outcome_variable ~., data = x) %>%
                       broom::tidy() %>%
                       dplyr::filter(term == "V2") %>%
                       dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
                       dplyr::select(variance) %>%
                       unlist())

stopCluster(cl)

true_vars <- do.call(rbind, true_var) %>%
  as.data.frame() %>%
  mutate(i_th_dataset = 1:100)
```

# Organizing Results 
```{r}

```


# Analysis of Results

```{r, warning=FALSE}
# Read in results from previous MC for jackknife v_1_bias_corrected
df <- mc_res %>%
  do.call(rbind, .) %>%
  tibble::remove_rownames(.) %>%
  cbind()

# CI coverage probability is very high, maybe consider traditional CI instead of percentile? 
round((sum(df$true_var < df$UB & df$true_var > df$LB) / nrow(df))*100,2)

ggplot(data = df,
           aes(point_estimate - true_var$estimate)) + 
      geom_density(aes(y = ..density..)) + 
      theme(axis.title.y = element_blank(), 
            panel.spacing=unit(1.5,"lines")) + 
      theme_bw() + 
      theme(axis.title.y = element_blank(), 
            panel.spacing=unit(1.5,"lines"), 
            strip.text = element_text(
              size = 9)) + 
      xlab("Bias of variance estimator") + 
      ggpubr::stat_overlay_normal_density(color = "red", linetype = "dashed")

ggplot(df, aes(x = c(1:nrow(df)), y = point_estimate)) + 
      geom_point(aes(color = "red")) + 
      geom_errorbar(aes(ymin = LB, ymax = UB, alpha = 1)) + 
      coord_flip() + 
      theme_bw() + 
      labs(
        x = latex2exp::TeX("$i^{th} dataset"), 
        y = latex2exp::TeX("$\\widehat{\\sigma^2}")
      ) + 
      theme(legend.position="none") + 
      geom_hline(yintercept = df$true_variance)
```
