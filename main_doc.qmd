---
title: "main_doc"
format: html
editor: visual
---
      
# Dependencies

```{r, message=FALSE}
library(mice)
library(MASS)
library(furrr)
library(purrr)
library(tidyverse)
library(parallel)

num_cores <- parallel::detectCores() - 1

number_of_imps <- 2

number_of_its <- 1
```

# Data Simulation

## Defining data simulation function

```{r}
data_generator <- function(sample_size, seed, prop, desired_data){
  set.seed(seed)
  cor_mat <- matrix(c(1, 0.5, 0.5, 
                      0.5, 1, 0.5, 
                      0.5, 0.5, 1), nrow = 3, ncol = 3)
  
  mean_vec <- c(1, 1, 1)
  
  covariates <- as.data.frame(mvrnorm(sample_size, mu = mean_vec, Sigma = cor_mat))
  
  outcome_variable <- 4 + 
    2*covariates$V1 + 
    5*covariates$V2 +  
    8*covariates$V3 +
    rnorm(sample_size, 0, 5)
  
  data_complete <- cbind(outcome_variable, covariates)
  
  data_w_missing <- 
    suppressWarnings(
      ampute(data_complete, prop = prop, mech = "MAR", patterns = c(0, 1, 1, 1))$amp
    )
  
  return(get(desired_data))
}

```

## Creating data for simulation

```{r, eval = TRUE}
set.seed(971423)
N = 100
sample_size = 1000
seed = 1:N
prop = 0.3
desired_data = "data_w_missing"

# Create list of data frames with missing 
cl <- makeCluster(num_cores)

list_of_sim_data <- mclapply(mc.cores = num_cores, seed, function(x) data_generator(sample_size, x, prop, desired_data))

stopCluster(cl)

df_w_mis <- list_of_sim_data[[10]]

uncon_pred_mat <- make.predictorMatrix(df_w_mis)

uncon_pred_mat[,4] <- 0
```

# Estimator Code

```{r}
choose.int <- function(x, n, k) {
  if(n <= k) return(rep(TRUE, k))
  u <- choose(n-1, k-1)
  pick <- x < u
  if (pick) y <- choose.int(x, n-1, k-1) else y <- choose.int(x-u, n-1, k)
  return(c(pick, y))
}
# n is the sample size of the full datasets (n = 1000 for us)
# k is the # of observations we would like to keep from n = 1000 
# The value inside sapply is the # of subsamples we would like. 
n <- 1000; k <- 100

length_sample_vec <- choose(n, k)

if (choose(n, k) > 1e15) {
  length_sample_vec = 1e15
} else
{
  length_sample_vec = choose(n, k)
}

# Column is index for what to include/exclude 

# If choose(n,k) is a big number (~1e15), code fails. 
sample <- sapply(sample.int(length_sample_vec, 500, replace = FALSE, )-1,
                 choose.int, n=n, k=k)

jackknife_estimator <- function(df_w_mis){
  
  drop_d_subsamples <- map(1:ncol(sample),
                           ~df_w_mis[sample[,.x],])
  
  uncon_pred_mat <- make.predictorMatrix(df_w_mis)
  
  uncon_pred_mat[,4] <- 0
  
  # Creates n subsamples of n = n-1 
  
  contains_na <- map_lgl(drop_d_subsamples, ~any(is.na(.x)))
  
  for (i in 1:length(drop_d_subsamples)) {
    if (contains_na[[i]] == TRUE) {
      drop_d_subsamples[[i]] <-
        mice(
          drop_d_subsamples[[i]],
          seed = 123,
          m = number_of_imps,
          method = "pmm",
          print = FALSE,
          maxit = number_of_its,
          predictorMatrix = uncon_pred_mat
        )
    } else{
      drop_d_subsamples[[i]] <- drop_d_subsamples[[i]]
    }
  }
  
  # Logical test to determine presence of MICE objects 
  is_mice <- vector("logical", length = length(drop_d_subsamples))
  
  for(i in 1:length(drop_d_subsamples)){
    is_mice[[i]] <- ifelse(class(drop_d_subsamples[[i]]) == "mids", TRUE, FALSE)
  }
  
  # Applies appt analysis depending on type 
  analysis_vector <- vector("numeric", length = length(drop_d_subsamples))
  
  for(i in 1:length(drop_d_subsamples)){
    sample_size <- 1000
    if(is_mice[[i]] == TRUE){
      analysis_vector[[i]] <- drop_d_subsamples[[i]] %>%
        mice::complete("long") %>%
        lm(outcome_variable ~ V1 + V2 + V3, data = .) %>%
        summary() %>%
        broom::tidy() %>%
        dplyr::filter(term == "V1") %>%
        dplyr::select(estimate) %>%
        unlist()
    } else{
      analysis_vector[[i]] <- drop_d_subsamples[[i]] %>%
        lm(formula = outcome_variable ~ V1 + V2 + V3, data = .) %>%
        summary() %>%
        broom::tidy() %>%
        dplyr::filter(term == "V1") %>%
        dplyr::select(estimate) %>%
        unlist()
    }
    
  }
  
  # Jackknife point estimate
  # jittered_analysis_vector <- jitter(analysis_vector)
  point_estimate_jackknife <- mean(analysis_vector)
  
  # Quantile CI
  UB <- quantile(analysis_vector, 0.975)
  LB <- quantile(analysis_vector, 0.025)
  
  return(data.frame("point_estimate" = point_estimate_jackknife, 
                   "UB" = UB, 
                   "LB" = LB) %>% tibble::remove_rownames())
}
```

# Small Monte Carlo for Jackknife Estimator

```{r, eval = FALSE}
# Singular test run 

jackknife_estimator(df_w_mis)

# Small sim to see effect of k 

possible_ks <- seq.int(100, 200, 1)

k_test_fun <- function(poss_k){
  n <- 1000; k <- poss_k
  
  length_sample_vec <- choose(n, k)
  
  if (choose(n, k) > 1e15) {
    length_sample_vec = 1e15
  } else
  {
    length_sample_vec = choose(n, k)
  }
  
  sample <- sapply(sample.int(length_sample_vec, 500, replace = FALSE, )-1,
                   choose.int, n=n, k=k)
  
  
  drop_d_subsamples <- map(1:ncol(sample),
                           ~df_w_mis[sample[,.x],])
  
  # Creates n subsamples of n = n-1 
  
  contains_na <- map_lgl(drop_d_subsamples, ~any(is.na(.x)))
  for (i in 1:length(drop_d_subsamples)) {
    if (contains_na[[i]] == TRUE) {
      drop_d_subsamples[[i]] <-
        mice(
          drop_d_subsamples[[i]],
          seed = 123,
          m = number_of_imps,
          method = "pmm",
          print = FALSE,
          maxit = number_of_its,
          predictorMatrix = uncon_pred_mat
        )
    } else{
      drop_d_subsamples[[i]] <- drop_d_subsamples[[i]]
    }
  }
  
  # Logical test to determine presence of MICE objects 
  is_mice <- vector("logical", length = length(drop_d_subsamples))
  
  for(i in 1:length(drop_d_subsamples)){
    is_mice[[i]] <- ifelse(class(drop_d_subsamples[[i]]) == "mids", TRUE, FALSE)
  }
  
  # Applies appt analysis depending on type 
  analysis_vector <- vector("numeric", length = length(drop_d_subsamples))
  
  for(i in 1:length(drop_d_subsamples)){
    if(is_mice[[i]] == TRUE){
      analysis_vector[[i]] <- drop_d_subsamples[[i]] %>%
        mice::complete("long") %>%
        lm(outcome_variable ~ V1 + V2 + V3, data = .) %>%
        summary() %>%
        broom::tidy() %>%
        dplyr::filter(term == "V1") %>%
        dplyr::select(estimate) %>%
        unlist()
    } else{
      analysis_vector[[i]] <- drop_d_subsamples[[i]] %>%
        lm(formula = outcome_variable ~ V1 + V2 + V3, data = .) %>%
        summary() %>%
        broom::tidy() %>%
        dplyr::filter(term == "V1") %>%
        dplyr::select(estimate) %>%
        unlist()
    }
    
  }
  
  # Jackknife point estimate
  # jittered_analysis_vector <- jitter(analysis_vector)
  point_estimate_jackknife <- mean(analysis_vector)
  
  # Quantile CI
  UB <- quantile(analysis_vector, 0.975)
  LB <- quantile(analysis_vector, 0.025)
  
  return(data.frame("point_estimate" = point_estimate_jackknife, 
                    "UB" = UB, 
                    "LB" = LB) %>% tibble::remove_rownames())
}

cl <- makeCluster(num_cores)

k_res <- mclapply(mc.cores = num_cores, possible_ks, k_test_fun)

stopCluster(cl)

k_res_df <- do.call(rbind, k_res) %>%
  as.data.frame()
```

## Unix

```{r}
cl <- makeCluster(num_cores)

jackknife_var_est <- mclapply(mc.cores = num_cores, list_of_sim_data, function(x) jackknife_estimator(x))

stopCluster(cl)

jackknife_var_estimates <- do.call(rbind, jackknife_var_est) %>%
  as.data.frame()
```

## Windows

```{r, eval = FALSE}
library(future.apply)
plan(multisession)

jackknife_var_est <- future_lapply(list_of_sim_data, function(x) jackknife_estimator(x), future.seed=TRUE)
```

# Small Monte Carlo for Rubin's Rules

```{r, eval = TRUE}
# If using windows, modify using code above 

cl <- makeCluster(num_cores)

rubin_var_estimates <- mclapply(mc.cores = num_cores,list_of_sim_data, function(x) 
  mice(x, seed = 123, print = FALSE, method = "pmm", m = number_of_imps, maxit = number_of_its,  predictorMatrix = uncon_pred_mat) %>%
    mice::complete("long") %>%
    group_by(.imp) %>%
    do(model = lm(outcome_variable ~ V1 + V2 + V3, data = .)) %>%
    as.list() %>%
    .[[-1]] %>%
    pool() %>%
    summary(conf.int = TRUE) %>%
    as.data.frame() %>%
    dplyr::filter(term == "V1") %>%
    dplyr::select(c(estimate, "2.5 %", "97.5 %")) %>%
    dplyr::rename(point_estimate_rub = estimate, 
                  LB_rub = "2.5 %", 
                  UB_rub = "97.5 %"))
stopCluster(cl)

rubin_var_estimates <- do.call(rbind, rubin_var_estimates) %>%
  as.data.frame()

# Rubin's coverage
round((sum(2 < rubin_var_estimates$UB_rub & 2 > rubin_var_estimates$LB_rub) / nrow(rubin_var_estimates))*100,2)
```

# Organizing Results

```{r}
jackknife_estimate <- do.call(rbind,jackknife_var_est)

combined_results <- cbind(true_var = 2, rubin_var_estimates, jackknife_estimate) 

rubin_width <- combined_results$UB_rub - combined_results$LB_rub
jackknife_width <- combined_results$UB - combined_results$LB

combined_results <- cbind(combined_results, rubin_width, jackknife_width)

# % of samples for which the rubin CI is wider than the jackknife one. 
sum(combined_results$rubin_width > combined_results$jackknife_width) / N * 100
```

# Analysis of Results

```{r}
# Jackknife coverage probability
round((sum(combined_results$true_var < combined_results$UB & combined_results$true_var > combined_results$LB) / nrow(combined_results))*100,2)

reshape2::melt(combined_results[c("point_estimate_rub", "point_estimate")]) %>%
  ggplot(data = .,
         aes(x = value, fill = variable, color = variable)) + 
  geom_density(aes(y = ..density..), alpha = 0.25) + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines")) + 
  theme_bw() + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines"), 
        strip.text = element_text(
          size = 9)) + 
  xlab("Beta_hat_1")

combined_results %>%
  mutate("jackknife_bias" = point_estimate - 2, 
         "rubin_bias" = point_estimate_rub - 2) %>%
  dplyr::select("jackknife_bias", "rubin_bias") %>%
  reshape2::melt() %>%
  ggplot(data = .,
         aes(x = value, fill = variable, color = variable)) + 
  geom_density(aes(y = ..density..), alpha = 0.25) + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines")) + 
  theme_bw() + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines"), 
        strip.text = element_text(
          size = 9)) + 
  xlab("Bias of the Jackknife Estimator")

ggplot(combined_results, aes(x = c(1:nrow(combined_results)))) + 
      geom_point(aes(y = point_estimate, color = "red")) +
      geom_point(aes(y = 2, alpha = 0.1)) + 
      geom_errorbar(aes(ymin = LB, ymax = UB, alpha = 1)) + 
      coord_flip() + 
      theme_bw() + 
      labs(
        x = latex2exp::TeX("$i^{th} dataset"), 
        y = latex2exp::TeX("$\\widehat{\\sigma^2}")
      ) + 
      theme(legend.position="none") + 
      geom_hline(yintercept = combined_results$true_var)
```
