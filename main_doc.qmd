---
title: "main_doc"
format: html
editor: visual
---

# Dependencies

```{r, message=FALSE}
library(mice)
library(MASS)
library(furrr)
library(purrr)
library(tidyverse)
```

# Data Simulation

## Defining data simulation function

```{r}
data_generator <- function(sample_size, seed, prop, desired_data){
  
  set.seed(seed)
  cor_mat <- matrix(c(1, 0.5, 0.5, 
                      0.5, 1, 0.5, 
                      0.5, 0.5, 1), nrow = 3, ncol = 3)
  
  mean_vec <- c(1, 1, 1)
  
  covariates <- as.data.frame(mvrnorm(sample_size, mu = mean_vec, Sigma = cor_mat))
  covariates$V3 <- ifelse(covariates$V3 > 0, 1, 0)
  
  outcome_variable <- 1 + 
    0.2*covariates$V1 + 
    0.5*covariates$V2 +  
    0.8*covariates$V3 +
    rlnorm(sample_size,0,1)
  
  data_complete <- cbind(outcome_variable, covariates)
  data_complete$V3 <- as.factor(data_complete$V3)
  
  data_w_missing <- suppressWarnings(ampute(data_complete, prop = prop, mech = "MAR", patterns = c(0, 1, 1, 1))$amp)
  
  data_w_missing$V3 <- as.factor(ifelse(data_w_missing$V3 == 1, 0, 1))
  
  return(get(desired_data))
}
```

## Creating dataframe of inputs

```{r, eval = FALSE}
set.seed(971423)
N = 1e3 
# 1e3 iterations of 4x4 unique conditions. 

p_mis_vec <- c(rep(0.10, N), rep(0.30, N), rep(0.50, N), rep(0.70, N))

sample_size_vec <- c(rep(1e2, N/4), rep(1e3, N/4), rep(1e4, N/4), rep(1e5, N/4))

seed_vec <- seq.int(from = 1, to = N*4)

sample_size_vec_repeated <- rep.int(sample_size_vec, 4)

param_df <- data.frame(sample_size = sample_size_vec_repeated, prop = p_mis_vec, seed = seed_vec)

plan(multisession, workers = parallel::detectCores()-1)

sim_dfs <- future_pmap(param_df, data_generator, .progress = TRUE)
```

### Small dataset for general use

```{r}
set.seed(123)
df_complete <- data_generator(1000, 213, 0.30, "data_complete")

qqnorm(df_complete$outcome_variable, pch = 0.1, frame = FALSE)
qqline(df_complete$outcome_variable, col = "steelblue", lwd = 2)

df_w_mis <- data_generator(1000, 213, 0.01, "data_w_missing")
```

# Estimator Code 
```{r}
# Creates n subsamples of n = n-1 
subsamples <- vector("list", nrow(df_w_mis))
for(i in 1:nrow(df_w_mis)){
  subsamples[[i]] <- df_w_mis[-i,]}

contains_na <- map_lgl(subsamples, ~any(is.na(.x)))

for(i in 1:length(subsamples)){
  if(contains_na[[i]] == TRUE){
    subsamples[[i]] <- mice(subsamples[[i]], m = 5, method = "pmm", print=FALSE)
  } else{
    subsamples[[i]] <- subsamples[[i]]
  }
}

# Logical test to determine presence of MICE objects 
is_mice <- vector("logical", length = length(subsamples))

for(i in 1:length(subsamples)){
  is_mice[[i]] <- ifelse(class(subsamples[[i]]) == "mids", TRUE, FALSE)
}

# Applies appt analysis depending on type 
analysis_list <- vector("list", length = length(subsamples))

for(i in 1:length(subsamples)){
  sample_size <- nrow(complete(subsamples[[i]],1))
  if(is_mice[[i]] == TRUE){
    analysis_list[[i]] <- subsamples[[i]] %>%
    mice::complete("all") %>%
    map(lm, formula = outcome_variable ~ V1 + V2 + V3) %>%
    pool() %>%
    broom::tidy() %>%
    dplyr::filter(term == "V2") %>%
    dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
    dplyr::select(estimate, variance) 
    } else{
    analysis_list[[i]] <- subsamples[[i]] %>%
      lm(formula = outcome_variable ~ V1 + V2 + V3 , data = .) %>%
      broom::tidy() %>%
      dplyr::filter(term == "V2") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select(estimate, variance)
  }

}

# Flattens resulting list
analysis_df <- do.call(rbind, analysis_list)

# Jackknife point estimate
point_estimate_jackknife <- mean(analysis_df$estimate)

UB <- quantile(analysis_df$estimate, 0.95)
LB <- quantile(analysis_df$estimate, 0.05)

print(data.frame("point_estimate" = point_estimate_jackknife, 
                 "UB" = UB, 
                 "LB" = LB) %>% tibble::remove_rownames())

# Estimate that is likely biased, since it does not use resampling 
estimated_value <- df_w_mis %>%
  mice(seed = 123, print = FALSE, method = "pmm") %>%
  complete("all") %>%
  lapply(lm, formula = outcome_variable ~ V1 + V2 + V3) %>%
  pool %>%
  broom::tidy() %>%
  dplyr::filter(term == "V2") %>%
  dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
  dplyr::select(estimate, variance)
```

# Small MC Codes 

```{r}
library(parallel)
cl <- makeCluster(4)
mc_res <- mclapply(mc.cores = 4, c(1:10), function(x) jackknife_estimator(df_w_mis, seed = x))

# Code below is appt. for windows machines, but slower than the one above. 
library(future.apply)
plan(multisession)

mc_res <- future_lapply(c(1:50), function(x) jackknife_v1_bias_corrected(df_w_mis, seed = x))

# Saving is same for both
saveRDS(mc_res, "mc_res_v1_bias_corrected.rds")
```

# Analysis of Results 
```{r, warning=FALSE}
# Read in results from previous MC for jackknife v_1_bias_corrected
df <- mc_res %>%
  do.call(rbind, .) %>%
  tibble::remove_rownames(.) %>%
  cbind()

# CI coverage probability is very high, maybe consider traditional CI instead of percentile? 
round((sum(df$true_var < df$UB & df$true_var > df$LB) / nrow(df))*100,2)

ggplot(data = df,
           aes(point_estimate - true_var$estimate)) + 
      geom_density(aes(y = ..density..)) + 
      theme(axis.title.y = element_blank(), 
            panel.spacing=unit(1.5,"lines")) + 
      theme_bw() + 
      theme(axis.title.y = element_blank(), 
            panel.spacing=unit(1.5,"lines"), 
            strip.text = element_text(
              size = 9)) + 
      xlab("Bias of variance estimator") + 
      ggpubr::stat_overlay_normal_density(color = "red", linetype = "dashed")

ggplot(df, aes(x = c(1:nrow(df)), y = point_estimate)) + 
      geom_point(aes(color = "red")) + 
      geom_errorbar(aes(ymin = LB, ymax = UB, alpha = 1)) + 
      coord_flip() + 
      theme_bw() + 
      labs(
        x = latex2exp::TeX("$i^{th} dataset"), 
        y = latex2exp::TeX("$\\widehat{\\sigma^2}")
      ) + 
      theme(legend.position="none") + 
      geom_hline(yintercept = df$true_variance)
```
