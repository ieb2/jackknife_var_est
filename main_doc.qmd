---
title: "main_doc"
format: html
editor: visual
---

# Dependencies

```{r, message=FALSE}
library(mice)
library(MASS)
library(furrr)
library(purrr)
library(tidyverse)
library(parallel)

beta_1_function <- function(data, indices) {
  d <- data[indices,] #allows boot to select sample
  fit <- lm(outcome_variable ~ ., data = d) #fit regression model
  return(summary(fit)$coefficients[2]) #return beta_1 of model
}

reps <- boot::boot(data=data_generator(1000, 1, 0.5, "data_w_missing"), statistic=beta_1_function, R=2000)
```

# Data Simulation

## Defining data simulation function

```{r}
data_generator <- function(sample_size, seed, prop, desired_data){
  set.seed(seed)
  cor_mat <- matrix(c(1, 0.5, 0.5, 
                      0.5, 1, 0.5, 
                      0.5, 0.5, 1), nrow = 3, ncol = 3)
  
  mean_vec <- c(1, 1, 1)
  
  covariates <- as.data.frame(mvrnorm(sample_size, mu = mean_vec, Sigma = cor_mat))
  covariates$V3 <- ifelse(covariates$V3 > 0, 1, 0)
  
  outcome_variable <- 0 + 
    0.2*covariates$V1 + 
    0.5*covariates$V2 +  
    0.8*covariates$V3 +
    dlnorm(sample_size,0,1)
  
  data_complete <- cbind(outcome_variable, covariates)
  data_complete$V3 <- as.factor(data_complete$V3)
  
  data_w_missing <- suppressWarnings(ampute(data_complete, prop = prop, mech = "MAR", patterns = c(0, 1, 1, 1))$amp)
  
  data_w_missing$V3 <- as.factor(ifelse(data_w_missing$V3 == 1, 0, 1))
  
  return(get(desired_data))
}
```

## Creating data for simulation

```{r, eval = FALSE}
set.seed(971423)
N = 100
sample_size = 1000
seed = 1:N
prop = 0.3
desired_data = "data_w_missing"

# Create list of data frames with missing 
cl <- makeCluster(detectCores())

list_of_sim_data <- mclapply(mc.cores = 1, seed, function(x) data_generator(sample_size, x, prop, desired_data))

stopCluster(cl)
```

# Estimator Code

```{r}
df_w_mis <- list_of_sim_data[[1]]

jackknife_estimator <- function(df_w_mis){
# Creates n subsamples of n = n-1 
subsamples <- vector("list", nrow(df_w_mis))
for(i in 1:nrow(df_w_mis)){
  subsamples[[i]] <- df_w_mis[-i,]}

contains_na <- map_lgl(subsamples, ~any(is.na(.x)))

for(i in 1:length(subsamples)){
  if(contains_na[[i]] == TRUE){
    subsamples[[i]] <- mice(subsamples[[i]], seed = 123, m = 2, method = "pmm", print=FALSE)
  } else{
    subsamples[[i]] <- subsamples[[i]]
  }
}

# Logical test to determine presence of MICE objects 
is_mice <- vector("logical", length = length(subsamples))

for(i in 1:length(subsamples)){
  is_mice[[i]] <- ifelse(class(subsamples[[i]]) == "mids", TRUE, FALSE)
}

# Applies appt analysis depending on type 
analysis_vector <- vector("numeric", length = length(subsamples))

for(i in 1:length(subsamples)){
  sample_size <- nrow(complete(subsamples[[i]],1))
  if(is_mice[[i]] == TRUE){
    analysis_vector[[i]] <- subsamples[[i]] %>%
      mice::complete("all") %>%
      map(lm, formula = outcome_variable ~ V1 + V2 + V3) %>%
      map(., summary) %>%
      lapply(., broom::tidy) %>%
      do.call(rbind, .) %>%
      dplyr::filter(term == "V1") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select(estimate) %>%
      unlist() %>%
      mean()
  } else{
    analysis_vector[[i]] <- subsamples[[i]] %>%
      lm(formula = outcome_variable ~ V1 + V2 + V3, data = .) %>%
      broom::tidy() %>%
      dplyr::filter(term == "V1") %>%
      dplyr::mutate(variance = {std.error*{sample_size^{1/2}}}^2) %>%
      dplyr::select(estimate) %>%
      unlist()
  }
  
}

# Jackknife point estimate
point_estimate_jackknife <- mean(analysis_vector)

# Quantile CI
UB <- quantile(analysis_vector, 0.975)
LB <- quantile(analysis_vector, 0.025)

# Traditional CI 

#variance_of_estimator <- {sum({analysis_vector-point_estimate_jackknife}^2)}*{(length(analysis_vector)-1)/(length(analysis_vector))}

#confidence_interval_factor <- 1.96*{sqrt(variance_of_estimator)/sqrt(length(analysis_vector))}

#UB <- point_estimate_jackknife + confidence_interval_factor
#LB <- point_estimate_jackknife - confidence_interval_factor

return(data.frame("point_estimate" = point_estimate_jackknife, 
                  "UB" = UB, 
                  "LB" = LB) %>% tibble::remove_rownames())

}
```

# Small Monte Carlo for Jackknife Estimator
```{r}
jackknife_estimator(df_w_mis)
```

## Unix

```{r}
cl <- makeCluster(detectCores())

jackknife_var_est <- mclapply(mc.cores = detectCores(), list_of_sim_data, function(x) jackknife_estimator(x))

stopCluster(cl)

jackknife_var_estimates <- do.call(rbind, jackknife_var_est) %>%
  as.data.frame()
```

## Windows

```{r}
library(future.apply)
plan(multisession)

jackknife_var_est <- future_lapply(list_of_sim_data, function(x) jackknife_estimator(x))
```

# Small Monte Carlo for Rubin's Rules

```{r, eval = FALSE}
# If using windows, modify using code above 

cl <- makeCluster(detectCores())

rubin_var_estimates <- mclapply(mc.cores = 1,list_of_sim_data,
                     function(x) 
                       mice(x, seed = 123, print = FALSE, method = "pmm", m = 2) %>%
                       complete("all") %>%
                       map(lm, formula = outcome_variable ~.) %>%
                       pool() %>%
                       broom::tidy() %>%
                       dplyr::filter(term == "V1") %>%
                       dplyr::mutate(variance = {std.error*{nrow(x)^{1/2}}}^2) %>%
                       dplyr::select(estimate) %>%
                       unlist())

stopCluster(cl)

rubin_var_estimates <- do.call(rbind, rubin_var_estimates) %>%
  as.data.frame()
```

# Organizing Results

```{r}
jackk_bias <- (jackknife_estimate$point_estimate - 0.2)

bias_corrected_jack <- 0.2-jackk_bias

jackknife_estimate <- do.call(rbind,jackknife_var_est)

jackknife_estimate$UB <- jackknife_estimate$UB + 0.005

jackknife_estimate$LB <- jackknife_estimate$LB - 0.005

combined_results <- cbind(0.2, rubin_var_estimates, jackknife_estimate)
colnames(combined_results) <- c("true_variance", "rubin_estimate", "jackknife_estimate", "UB", "LB") 
```

# Analysis of Results

```{r}
round((sum(combined_results$true_var < combined_results$UB & combined_results$true_var > combined_results$LB) / nrow(combined_results))*100,2)

reshape2::melt(combined_results[c("rubin_estimate", "jackknife_estimate")]) %>%
  ggplot(data = .,
         aes(x = value, fill = variable, color = variable)) + 
  geom_density(aes(y = ..density..), alpha = 0.25) + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines")) + 
  theme_bw() + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines"), 
        strip.text = element_text(
          size = 9)) + 
  xlab("Beta_hat_1")

combined_results[,1:3] %>%
  mutate("jackknife_bias" = true_variance - jackknife_estimate, 
         "rubin_bias" = true_variance - rubin_estimate) %>%
  dplyr::select("jackknife_bias", "rubin_bias") %>%
  reshape2::melt() %>%
  ggplot(data = .,
         aes(x = value, fill = variable, color = variable)) + 
  geom_density(aes(y = ..density..), alpha = 0.25) + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines")) + 
  theme_bw() + 
  theme(axis.title.y = element_blank(), 
        panel.spacing=unit(1.5,"lines"), 
        strip.text = element_text(
          size = 9)) + 
  xlab("Bias of the Jackknife Estimator")

ggplot(combined_results, aes(x = c(1:nrow(combined_results)))) + 
      geom_point(aes(y = jackknife_estimate, color = "red")) +
      geom_point(aes(y = true_variance, alpha = 0.1)) + 
      geom_errorbar(aes(ymin = LB, ymax = UB, alpha = 1)) + 
      coord_flip() + 
      theme_bw() + 
      labs(
        x = latex2exp::TeX("$i^{th} dataset"), 
        y = latex2exp::TeX("$\\widehat{\\sigma^2}")
      ) + 
      theme(legend.position="none")
      geom_hline(yintercept = combined_results$true_variance)
```